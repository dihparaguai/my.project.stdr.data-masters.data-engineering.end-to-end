services:
  minio:  # Service name
    image: minio/minio:RELEASE.2025-09-07T16-13-09Z-cpuv1   # Official MinIO image from Docker Hub
    container_name: minio   # Name to use in "docker exec -it minio bash"
    
    command: server /data --console-address ":9001"   # Start MinIO server, specifying data location and console port
    
    ports:  # (Accept only list format)
      - "9000:9000"  # API port to connect Python/Spark scripts (local:container)
      - "9001:9001"  # Web console port to access in browser (local:container)
    
    environment:  # (Accept key-value and list formats)
      MINIO_ROOT_USER: ${MINIO_ROOT_USER}          # Admin user
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD}  # Admin password
    
    volumes:
      - ./data:/data  # Maps the local folder ./data/minio to the container's /data folder for persistent storage (local:container) [Using local because the data can be accessed and used]

    healthcheck:  # Check if MinIO is running (not only if its port is open) [response code 200 means healthy]
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"] # Command to test health (you can use: "docker exec -it minio curl -I http://localhost:9000/minio/health/live" to test it manually)
      start_period: 10s     # Time to wait after starting the container before starting health checks
      interval: 10s         # Time between each check
      timeout: 10s          # Time to wait for a response before considering it failed
      retries: 5            # Number of retries before considering the service unhealthy

# ================================================================

  postgres:
    image: postgres:15
    container_name: postgres
    
    environment: # Set environment variables for Airflow connect to Postgres
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB} # Database name
    
    ports:
      - "5432:5432"
    
    volumes:
      - postgres_data:/var/lib/postgresql/data # Maps a Docker volume for persistent storage (volume:container) [Using volume because the data is only used by Postgres]

    healthcheck: # [response "accepting connections" means healthy]
      test: ["CMD-SHELL", "pg_isready -U airflow -d airflow"] # Command to test health (you can use: "docker exec -it postgres pg_isready -U airflow -d airflow" to test it manually)
      start_period: 10s
      interval: 10s
      timeout: 10s
      retries: 5

# ================================================================

  airflow:
    build:
      context: .
      dockerfile: Dockerfile
    image: airflow:d-eng
    container_name: airflow
    
    depends_on: # Wait for these services to be healthy before starting Airflow
      postgres:
        condition: service_healthy
      minio:
        condition: service_healthy

    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor # Run tasks in parallel using in the same machine

      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB} # Connection string to Postgres to store Airflow metadata

      AIRFLOW__WEBSERVER__RBAC: "True"       # Enable "Role-Based Access Control (RBAC)" in webserver, allows creating users with different permissions (admin, user, viewer, etc)
      AIRFLOW__CORE__LOAD_EXAMPLES: "False"  # Disable example DAGs in webserver (set to "True" to enable)

      AWS_ACCESS_KEY_ID: ${MINIO_ROOT_USER}           # MinIO user
      AWS_SECRET_ACCESS_KEY: ${MINIO_ROOT_PASSWORD}   # MinIO password
      AWS_ENDPOINT_URL: http://minio:9000             # MinIO endpoint

      PYTHONPATH: /opt/airflow  # Add Airflow home to Python path to access modules/helpers easily

    volumes:
      - ./dags:/opt/airflow/dags                         # Airflow DAGs – workflow definitions (schedules, tasks)
      - ./jobs:/opt/airflow/jobs                         # Jobs – Spark or ETL scripts executed by DAGs
      - ./modules:/opt/airflow/modules                   # Modules – ETL logic (functions, classes, transformations)
      - ./helpers:/opt/airflow/helpers                   # Helpers – small utilities (logging, Spark session, configs)
      - airflow_logs:/opt/airflow/logs                   # Logs folder (Using volume because logs we don't need to access them directly from host)
      - airflow_plugins:/opt/airflow/plugins             # Plugins folder

    ports:
      - "8080:8080" # Airflow webserver port

    command: > # Run DB upgrade (ensure that database schema is up to date); create admin user; then start webserver and scheduler
      bash -c "
        airflow db upgrade &&

        airflow users create --username ${AIRFLOW_USER} --password ${AIRFLOW_PASSWORD} --firstname '${AIRFLOW_FIRSTNAME}' --lastname '${AIRFLOW_LASTNAME}' --role ${AIRFLOW_ROLE} --email ${AIRFLOW_EMAIL}

        airflow webserver & airflow scheduler
      "


# ================================================================

  spark-master:
    image: apache/spark:3.5.7-java17
    container_name: spark-master

    environment:
      - SPARK_HOME=/opt/spark           # Define Spark installation folder path
      - PYSPARK_PYTHON=python3          # Define Python interpreter for PySpark
      - PYSPARK_DRIVER_PYTHON=python3   # Define Python interpreter for Spark Driver
      - SPARK_HOME=/opt/spark
      - PATH=/opt/spark/bin:$PATH

    ports:
      - "7077:7077"   # Cluster communication
      - "8081:8081"   # Web UI

    volumes: # Folders shared with Airflow container
      - ./jobs:/opt/airflow/jobs                       
      - ./modules:/opt/airflow/modules                 
      - ./helpers:/opt/airflow/helpers                   

    command: > # Start Spark Master, define hostname, change default port from 8080 to 8081 (to avoid conflict with Airflow), and port 7077 for worker communication
      /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master
      
      --host spark-master
      
      --webui-port 8081
      --port 7077

# ================================================================

  spark-worker-1:
    image: apache/spark:3.5.7-java17
    container_name: spark-worker-1

    environment:
      - SPARK_HOME=/opt/spark
      - PYSPARK_PYTHON=python3
      - PYSPARK_DRIVER_PYTHON=python3

    volumes:
      - ./jobs:/opt/airflow/jobs                       
      - ./modules:/opt/airflow/modules                 
      - ./helpers:/opt/airflow/helpers   
    
    depends_on:
      - spark-master

    command: > # Start Spark Worker, connect to Spark Master
      /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker
      
      spark://spark-master:7077

# ================================================================

  spark-worker-2:
    image: apache/spark:3.5.7-java17
    container_name: spark-worker-2  

    environment:
      - SPARK_HOME=/opt/spark
      - PYSPARK_PYTHON=python3
      - PYSPARK_DRIVER_PYTHON=python3

    volumes:
      - ./jobs:/opt/airflow/jobs                       
      - ./modules:/opt/airflow/modules                 
      - ./helpers:/opt/airflow/helpers   
    
    depends_on:
      - spark-master

    command: >
      /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker
      
      spark://spark-master:7077

# ================================================================

volumes: # Define Docker volumes will be created and managed by Docker Compose
  postgres_data:
  airflow_logs:
  airflow_plugins: